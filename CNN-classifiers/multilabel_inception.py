# -*- coding: utf-8 -*-
"""multilabel_inception.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jJENfdyqQ24p-66-o802WJ8JGXUQkGWp
"""
from __future__ import print_function, division

import glob

# !unzip -u "/content/gdrive/MyDrive/research-project/dilbert-stuff/resized.zip" -d "/content/resized"
# !unzip -u "/content/gdrive/MyDrive/research-project/dilbert-stuff/de.zip" -d "/content/dilbert_equal"

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torch.autograd import Variable
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy
from torch import Tensor


from torch.utils.data import Dataset
from PIL import Image
import json
from sklearn.metrics import accuracy_score, f1_score, precision_score

import torch.utils.model_zoo as model_zoo
import torch.nn.functional as F
from typing import Callable, Any, Optional, Tuple, List



num_classes = 13

plt.ion()  

use_gpu = torch.cuda.is_available()
if use_gpu:
    print("Using CUDA")

class LabelsDataset(Dataset):
    def __init__(self, img_dir, labels_json, split='train',
                 base_size=64,
                 transform=None, target_transform=None):
        self.transform = transform
        self.norm = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        self.target_transform = target_transform
        # self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE

        # self.imsize = []
        # for i in range(cfg.TREE.BRANCH_NUM):
        #     self.imsize.append(base_size)
        #     base_size = base_size * 2

        self.data = []
        self.img_dir = img_dir

        self.split = split

        # self.classes = [0,1,2,3,4,5]

        with open(labels_json) as labels_file:

            self.labels_dict = json.load(labels_file)

            val_split = int(len(self.labels_dict) * 0.4)
            if split == "test":
                self.img_names = list(self.labels_dict.keys())[val_split:]
            elif split == "val":
                self.img_names = list(self.labels_dict.keys())[0:val_split]
            else:
                self.img_names = list(self.labels_dict.keys())

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):

        key = self.img_names[idx]
        img_path = os.path.join(self.img_dir, key)
        image = self.get_imgs(img_path, 299, transform=self.transform, normalize=self.norm)
        label = self.labels_dict[key]
        # if self.transform:
        #     image = self.transform(image)
        # if self.target_transform:
        #     label = self.target_transform(label)
        # return image, np.asarray(label, dtype=float)

        # print(label)
        label = torch.tensor(label)
        # print(label)
        # print(torch.max(label, 0)[1])
        return image, label


    def get_imgs(self, img_path, imsize, bbox=None,
                 transform=None, normalize=None):
        img = Image.open(img_path).convert('RGB')
        width, height = img.size

        # print(width)

        if transform is not None:
            img = transform(img)

        # re_img = transforms.Resize(imsize)(img)
        # re_img = img

        # print(re_img.size)
        # norm_image = normalize(re_img)

        return img

data_dir = '../input/kermany2018/oct2017/OCT2017 '
TRAIN = 'train'
VAL = 'val'
TEST = 'test'

# image_size = 224
image_size = 299

# inception Takes 299x299 images as input, so we resize all of them
data_transforms = {
    TRAIN: transforms.Compose([
        # randomly flip it horizontally. 
        # transforms.RandomResizedCrop(224),
        transforms.Resize(image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
    ]),
    VAL: transforms.Compose([
        # transforms.Resize(256),
        # transforms.CenterCrop(224),
        transforms.Resize(image_size),
        transforms.ToTensor(),
    ]),
    TEST: transforms.Compose([
        # transforms.Resize(256),
        # transforms.CenterCrop(224),
        transforms.Resize(image_size),
        transforms.ToTensor(),
    ])
}

# img_dir = '/content/resized/resized'
# train_json = "/content/gdrive/MyDrive/research-project/dilbert-stuff/labels_jsons/train_labels.json"
# test_json = "/content/gdrive/MyDrive/research-project/dilbert-stuff/labels_jsons/test_labels.json"

img_dir = '../data/dilbert/dilbert_equal/images/001.dilbert_equal'
train_json = "../data/dilbert/annotated-jsons/train_equal_labels.json"
test_json = "../data/dilbert/annotated-jsons/test_equal_labels.json"


train_dataset = LabelsDataset(img_dir, train_json, transform=data_transforms[TRAIN])
test_dataset = LabelsDataset(img_dir, test_json, transform=data_transforms[TEST], split="test")
val_dataset = LabelsDataset(img_dir, test_json, transform=data_transforms[VAL], split="val")


# image_datasets = {
#     x: datasets.ImageFolder(
#         os.path.join(data_dir, x), 
#         transform=data_transforms[x]
#     )
#     for x in [TRAIN, VAL, TEST]
# }

image_datasets = {TRAIN: train_dataset,
                  TEST: test_dataset,
                  VAL: val_dataset}

dataloaders = {
    x: torch.utils.data.DataLoader(
        image_datasets[x], batch_size=8,
        shuffle=True, num_workers=4
    )
    for x in [TRAIN, VAL, TEST]
}

dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}

for x in [TRAIN, VAL, TEST]:
    print("Loaded {} images under {}".format(dataset_sizes[x], x))
    
# print("Classes: ")
# class_names = image_datasets[TRAIN].classes
# print(image_datasets[TRAIN].classes)

def imshow(inp, title=None):
    inp = inp.numpy().transpose((1, 2, 0))
    # plt.figure(figsize=(10, 10))
    plt.axis('off')
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)

# def show_databatch(inputs, classes):
#     out = torchvision.utils.make_grid(inputs)
#     imshow(out, title=[class_names[x] for x in classes])

def show_databatch(inputs, labels):
    out1 = torchvision.utils.make_grid(inputs[0:4])
    out2 = torchvision.utils.make_grid(inputs[4:])
    imshow(out1, title=[x for x in labels[0:4]])
    imshow(out2, title=[x for x in labels[4:]])


# Get a batch of training data
inputs, labels = next(iter(dataloaders[TRAIN]))
show_databatch(inputs, labels)

def calculate_metrics(outputs, labels):
    out = np.array(outputs.detach().cpu().numpy())
    lab = np.array(labels.detach().cpu().numpy())

    threshold = 0.5
    pred = np.array(out > threshold, dtype=float)

    acc = accuracy_score(y_true=lab, y_pred=pred)
    f1_micro = f1_score(y_true=lab, y_pred=pred, average='micro')
    f1_weighted = f1_score(y_true=lab, y_pred=pred, average='weighted')
    f1_samples = f1_score(y_true=lab, y_pred=pred, average='samples')

    res = {"accuracy": acc, "f1 micro": f1_micro,
           "f1 weighted": f1_weighted, "f1 samples": f1_samples}

    return pred, res

def visualize_model(model, num_images=6):
    was_training = model.training
    
    # Set model for evaluation
    model.train(False)
    model.eval() 
    
    images_so_far = 0

    for i, data in enumerate(dataloaders[TEST]):
        inputs, labels = data
        size = inputs.size()[0]
        
        if use_gpu:
            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)
        else:
            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)
        
        outputs = model(inputs)
        
        # _, preds = torch.max(outputs.data, 1)

        preds, res = calculate_metrics(outputs, labels)

        predicted_labels = [preds[j] for j in range(inputs.size()[0])]

        print("Ground truth:")
        show_databatch(inputs.data.cpu(), labels.data.cpu())
        print("Prediction:")
        show_databatch(inputs.data.cpu(), predicted_labels)
        
        del inputs, labels, outputs, preds, predicted_labels
        torch.cuda.empty_cache()
        
        images_so_far += size
        if images_so_far >= num_images:
            break
        
    model.train(mode=was_training) # Revert model back to original training state

def eval_model(model, criterion):
    since = time.time()
    avg_loss = 0
    avg_acc = 0
    loss_test = 0
    acc_test = 0

    acc_arr = []
    f1_micro_arr = []
    f1_weighted_arr = []
    f1_samples_arr = []

    
    test_batches = len(dataloaders[TEST])
    print("Evaluating model")
    print('-' * 10)
    
    for i, data in enumerate(dataloaders[TEST]):
        if i % 100 == 0:
            print("\rTest batch {}/{}".format(i, test_batches), end='', flush=True)

        model.train(False)
        model.eval()
        inputs, labels = data

        if use_gpu:
            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)
        else:
            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)

        outputs = model(inputs)

        # print("out: ", outputs[0])
        # print("labels: ", labels[0])
        loss = criterion(outputs, labels.type(torch.float))


        # _, preds = torch.max(outputs.data, 1)
        # preds = [outputs[i] > threshold for i in range(inputs.size()[0])

        preds, res = calculate_metrics(outputs, labels)

        predicted_labels = [preds[j] for j in range(inputs.size()[0])]

        # loss = criterion(outputs, labels.type(torch.float))

        loss_test += loss.item()
        # acc_test += torch.sum(preds == labels.data)
        # acc_test += acc
        acc_arr.append(res["accuracy"])
        f1_micro_arr.append(res["f1 micro"])
        f1_weighted_arr.append(res["f1 weighted"])
        f1_samples_arr.append(res["f1 samples"])

        del inputs, labels, outputs, preds
        torch.cuda.empty_cache()
        
    avg_loss = loss_test / dataset_sizes[TEST]
    # avg_acc = acc_test / 72
    avg_acc = sum(acc_arr)/len(acc_arr)
    avg_f1_micro = sum(f1_micro_arr)/len(f1_micro_arr)
    avg_f1_weighted = sum(f1_weighted_arr)/len(f1_weighted_arr)
    avg_f1_samples = sum(f1_samples_arr)/len(f1_samples_arr)

    
    elapsed_time = time.time() - since
    print()
    print("Evaluation completed in {:.0f}m {:.0f}s".format(elapsed_time // 60, elapsed_time % 60))
    print("Avg loss (test): {:.4f}".format(avg_loss))
    print("Avg acc (test): {:.4f}".format(avg_acc))
    print("Avg f1 micro (test): {:.4f}".format(avg_f1_micro))
    print("Avg f1 weighted (test): {:.4f}".format(avg_f1_weighted))
    print("Avg f1 samples (test): {:.4f}".format(avg_f1_samples))
    print('-' * 10)


class ComicClassifier(nn.Module):

    def __init__(self, num_classes):
        super(ComicClassifier, self).__init__()

        self.transform_input = True

        conv_block = BasicConv2d
        inception_a = CustomInceptionA
        inception_b = CustomInceptionB
        inception_c = CustomInceptionC
        inception_aux = CustomInceptionAux
        inception_d = CustomInceptionD
        inception_e = CustomInceptionE

        # 32 to 16
        # N x 3 x 299 x 299
        self.Conv1 = conv_block(3, 16, kernel_size=3, stride=2)
        # 64 to 32
        # N x 16 x 149 x 149
        self.Conv2 = conv_block(16, 32, kernel_size=3)
        # N x 32 x 147 x 147

        self.Maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)
        # N x 32 x 73 x 73

        self.Conv3 = conv_block(32, 40, kernel_size=1)
        # N x 40 x 73 x 73
        self.Conv3b = conv_block(40, 96, kernel_size=3)
        # 96 x 71 x 71

        self.Maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)
        # N x 96 x 35 x 35

        # custom inception a

        self.Mixed5b = inception_a(96, pool_features=16)
        # N x 128 x 35 x 35

        # custom inception b
        self.Mixed6a = inception_b(128)
        # N x 320 x 17 x 17

        # custom inception c
        # self.Mixed_6b = inception_c(768, channels_7x7=128)
        self.Mixed6b = inception_c(320, channels_7x7=64)
        # N x 320 x 17 x 17

        # features are here 17x17 regions

        self.AuxLogits = inception_aux(320, num_classes)

        self.Mixed7a = inception_d(320)
        # 500 x 8 x 8

        self.mixed7b = inception_e(500)
        # 640 x 8 x 8
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout()
        self.fc = nn.Linear(960, num_classes)

    def _transform_input(self, x: Tensor) -> Tensor:
        if self.transform_input:
            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5
            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5
            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5
            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)
        return x

    def _forward(self, x: Tensor):

        # print(x.shape, "3 x 299 x 299")

        # N x 3 x 299 x 299
        x = self.Conv1(x)
        # N x 16 x 149 x 149
        x = self.Conv2(x)
        # N x 32 x 147 x 147
        x = self.Maxpool1(x)
        # N x 32 x 73 x 73
        x = self.Conv3(x)
        # N x 40 x 73 x 73
        x = self.Conv3b(x)
        # 96 x 71 x 71
        x = self.Maxpool2(x)
        # N x 96 x 35 x 35
        # custom inception a
        x = self.Mixed5b(x)
        # N x 128 x 35 x 35
        # custom inception b
        x = self.Mixed6a(x)
        # N x 320 x 17 x 17
        # custom inception c
        x = self.Mixed6b(x)
        # N x 320 x 17 x 17
        # print(x.shape, "320 x 17 x 17")

        # features are here 17x17 regions
        aux = self.AuxLogits(x)

        # custom inception d
        x = self.Mixed7a(x)
        # 500 x 8 x 8
        # custom inception e
        x = self.mixed7b(x)
        # 640 x 8 x 8
        # print(x.shape, "640 x 8 x 8")
        x = self.avgpool(x)
        # print(x.shape, "640 x 1 x 1")
        x = self.dropout(x)

        x = torch.flatten(x, 1)
        prediction = self.fc(x)

        return prediction, aux

    def forward(self, x: Tensor):
        x = self._transform_input(x)

        x, aux = self._forward(x)

        if self.training:
            return x, aux

        else:
            return x

class BasicConv2d(nn.Module):

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        **kwargs: Any
    ) -> None:
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)

    def forward(self, x: Tensor) -> Tensor:
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace=True)


class CustomInceptionA(nn.Module):

    def __init__(
        self,
        in_channels: int,
        pool_features: int = 16,
        conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(CustomInceptionA, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.branch1x1 = conv_block(in_channels, 32, kernel_size=1)

        self.branch5x5_1 = conv_block(in_channels, 24, kernel_size=1)
        self.branch5x5_2 = conv_block(24, 32, kernel_size=5, padding=2)

        self.branch3x3dbl_1 = conv_block(in_channels, 32, kernel_size=1)
        self.branch3x3dbl_2 = conv_block(32, 48, kernel_size=3, padding=1)
        self.branch3x3dbl_3 = conv_block(48, 48, kernel_size=3, padding=1)

        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class CustomInceptionB(nn.Module):

    def __init__(
        self,
        in_channels: int,
        conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(CustomInceptionB, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.branch3x3 = conv_block(in_channels, 144, kernel_size=3, stride=2)

        self.branch3x3dbl_1 = conv_block(in_channels, 32, kernel_size=1)
        self.branch3x3dbl_2 = conv_block(32, 48, kernel_size=3, padding=1)
        self.branch3x3dbl_3 = conv_block(48, 48, kernel_size=3, stride=2)

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch3x3 = self.branch3x3(x)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)

        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)

        outputs = [branch3x3, branch3x3dbl, branch_pool]
        # 144 + 48 + 128
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class CustomInceptionC(nn.Module):

    def __init__(
        self,
        in_channels: int,
        channels_7x7: int,
        conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(CustomInceptionC, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.branch1x1 = conv_block(in_channels, 80, kernel_size=1)

        c7 = channels_7x7
        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)
        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7_3 = conv_block(c7, 80, kernel_size=(7, 1), padding=(3, 0))

        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)
        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_5 = conv_block(c7, 80, kernel_size=(1, 7), padding=(0, 3))

        self.branch_pool = conv_block(in_channels, 80, kernel_size=1)

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch1x1 = self.branch1x1(x)

        branch7x7 = self.branch7x7_1(x)
        branch7x7 = self.branch7x7_2(branch7x7)
        branch7x7 = self.branch7x7_3(branch7x7)

        branch7x7dbl = self.branch7x7dbl_1(x)
        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class CustomInceptionD(nn.Module):

    def __init__(
        self,
        in_channels: int,
        conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(CustomInceptionD, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.branch3x3_1 = conv_block(in_channels, 64, kernel_size=1)
        self.branch3x3_2 = conv_block(64, 100, kernel_size=3, stride=2)

        self.branch7x7x3_1 = conv_block(in_channels, 64, kernel_size=1)
        self.branch7x7x3_2 = conv_block(64, 64, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7x3_3 = conv_block(64, 64, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7x3_4 = conv_block(64, 80, kernel_size=3, stride=2)

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = self.branch3x3_2(branch3x3)

        branch7x7x3 = self.branch7x7x3_1(x)
        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)
        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)
        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)

        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)
        outputs = [branch3x3, branch7x7x3, branch_pool]

        # 100 + 80 + 320
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class CustomInceptionE(nn.Module):

    def __init__(
        self,
        in_channels: int = 500,
        conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(CustomInceptionE, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.branch1x1 = conv_block(in_channels, 160, kernel_size=1)

        self.branch3x3_1 = conv_block(in_channels, 160, kernel_size=1)
        self.branch3x3_2a = conv_block(160, 160, kernel_size=(1, 3), padding=(0, 1))
        self.branch3x3_2b = conv_block(160, 160, kernel_size=(3, 1), padding=(1, 0))

        self.branch3x3dbl_1 = conv_block(in_channels, 200, kernel_size=1)
        self.branch3x3dbl_2 = conv_block(200, 160, kernel_size=3, padding=1)
        self.branch3x3dbl_3a = conv_block(160, 160, kernel_size=(1, 3), padding=(0, 1))
        self.branch3x3dbl_3b = conv_block(160, 160, kernel_size=(3, 1), padding=(1, 0))

        self.branch_pool = conv_block(in_channels, 160, kernel_size=1)

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch1x1 = self.branch1x1(x)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [
            self.branch3x3_2a(branch3x3),
            self.branch3x3_2b(branch3x3),
        ]
        branch3x3 = torch.cat(branch3x3, 1)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [
            self.branch3x3dbl_3a(branch3x3dbl),
            self.branch3x3dbl_3b(branch3x3dbl),
        ]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        # 160 + 160 + 160 + 160

        # 640
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)

def conv1x1(in_planes, out_planes, bias=False):
    "1x1 convolution with padding"
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,
                     padding=0, bias=bias)


# class CNN_IMAGE_ENCODER_SQUEEZENET(nn.Module):

class CustomInceptionAux(nn.Module):

    def __init__(
        self,
        in_channels: int,
        num_classes: int,
        conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(CustomInceptionAux, self).__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        self.conv0 = conv_block(in_channels, 40, kernel_size=1)
        self.conv1 = conv_block(40, 320, kernel_size=5)
        self.conv1.stddev = 0.01  # typeignore[assignment]
        self.fc = nn.Linear(320, num_classes)
        self.fc.stddev = 0.001  # typeignore[assignment]

    def forward(self, x: Tensor) -> Tensor:
        # N x 320 x 17 x 17
        x = F.avg_pool2d(x, kernel_size=5, stride=3)
        # N x 320 x 5 x 5
        x = self.conv0(x)
        # N x 40 x 5 x 5
        x = self.conv1(x)
        # N x 320 x 1 x 1
        # Adaptive average pooling
        x = F.adaptive_avg_pool2d(x, (1, 1))
        # N x 320 x 1 x 1
        x = torch.flatten(x, 1)
        # N x 320
        x = self.fc(x)
        # N x num_classes
        return x


class BasicConv2d(nn.Module):

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        **kwargs: Any
    ) -> None:
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)

    def forward(self, x: Tensor) -> Tensor:
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace=True)


aux_logits = True
custom_model = True
from_scratch = True

if not custom_model:

    incept = models.inception_v3(pretrained=False)

    # False is when we do not fine tune the existing layers
    if not from_scratch:
        for param in incept.parameters():
            param.require_grad = False

    num_ftrs = incept.AuxLogits.fc.in_features
    incept.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)

    num_ftrs = incept.fc.in_features
    incept.fc = nn.Linear(num_ftrs, num_classes)

    print(incept)

else:
    # use the class I created
    incept = ComicClassifier(num_classes)
    print(incept)

if use_gpu:
    incept.cuda() #.cuda() will move everything to the GPU side
    
# criterion = nn.CrossEntropyLoss()
criterion = nn.BCEWithLogitsLoss()

optimizer_ft = optim.SGD(incept.parameters(), lr=0.001, momentum=0.9)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

print("Test before training")
eval_model(incept, criterion)

visualize_model(incept) #test before training

def train_model(model, criterion, optimizer, scheduler, num_epochs=10, aux_logits=False):
    since = time.time()
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    
    avg_loss = 0
    avg_acc = 0
    avg_loss_val = 0
    avg_acc_val = 0

    avg_acc_arr_train = []
    avg_acc_arr_val = []
    
    train_batches = len(dataloaders[TRAIN])
    val_batches = len(dataloaders[VAL])
    
    for epoch in range(num_epochs):
        print("Epoch {}/{}".format(epoch, num_epochs))
        print('-' * 10)
        
        loss_train = 0
        loss_val = 0
        acc_train = 0
        acc_val = 0

        acc_arr = []
        acc_val_arr = []
        
        model.train(True)
        
        for i, data in enumerate(dataloaders[TRAIN]):
            if i % 100 == 0:
                print("\rTraining batch {}/{}".format(i, train_batches / 2), end='', flush=True)
                
            # Use half training dataset
            if i >= train_batches / 2:
                break
                
            inputs, labels = data
            
            if use_gpu:
                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())
            else:
                inputs, labels = Variable(inputs), Variable(labels)
            
            optimizer.zero_grad()
            
            if aux_logits:
                outputs, aux_outputs = model(inputs)

            # print("out: ", outputs[0])
            # print("labels: ", labels[0])
                loss1 = criterion(outputs, labels.type(torch.float))
                loss2 = criterion(aux_outputs, labels.type(torch.float))
                loss = loss1 + 0.4*loss2
            else:

                outputs = model(inputs)
                loss = criterion(outputs, labels.type(torch.float))
            
            loss.backward()
            optimizer.step()
            
            loss_train += loss.item()

            preds, res = calculate_metrics(outputs, labels.type(torch.float))

            predicted_labels = [preds[j] for j in range(inputs.size()[0])]

            # acc_train += acc
            acc_arr.append(res["accuracy"])
            
            del inputs, labels, outputs, preds
            torch.cuda.empty_cache()
        
        print()
        # * 2 as we only used half of the dataset
        avg_loss = loss_train * 2 / dataset_sizes[TRAIN]
        # avg_acc = acc_train * 2 / dataset_sizes[TRAIN]
        avg_acc = sum(acc_arr)/len(acc_arr)

        avg_acc_arr_train.append(avg_acc)
        
        model.train(False)
        model.eval()
            
        for i, data in enumerate(dataloaders[VAL]):
            if i % 100 == 0:
                print("\rValidation batch {}/{}".format(i, val_batches), end='', flush=True)
                
            inputs, labels = data
            
            if use_gpu:
                inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)
            else:
                inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)
            
            optimizer.zero_grad()
            
            outputs = model(inputs)

            # print("out: ", outputs[0])
            # print("labels: ", labels[0])
            loss = criterion(outputs, labels.type(torch.float))
            
            loss_val += loss.item()

            preds, res = calculate_metrics(outputs, labels)

            predicted_labels = [preds[j] for j in range(inputs.size()[0])]

            # acc_val += torch.sum(preds == labels.data)
            # acc_val += acc
            acc_val_arr.append(res["accuracy"])
            
            del inputs, labels, outputs, preds
            torch.cuda.empty_cache()
        
        avg_loss_val = loss_val / dataset_sizes[VAL]
        # avg_acc_val = acc_val / dataset_sizes[VAL]
        avg_acc_val = sum(acc_val_arr)/len(acc_val_arr)

        avg_acc_arr_val.append(avg_acc_val)

        
        print()
        print("Epoch {} result: ".format(epoch))
        print("Avg loss (train): {:.4f}".format(avg_loss))
        print("Avg acc (train): {:.4f}".format(avg_acc))
        print("Avg loss (val): {:.4f}".format(avg_loss_val))
        print("Avg acc (val): {:.4f}".format(avg_acc_val))
        print('-' * 10)
        print()
        
        if avg_acc_val > best_acc:
            best_acc = avg_acc_val
            best_model_wts = copy.deepcopy(model.state_dict())
        
    elapsed_time = time.time() - since
    print()
    print("Training completed in {:.0f}m {:.0f}s".format(elapsed_time // 60, elapsed_time % 60))
    print("Best acc: {:.4f}".format(best_acc))

    plt.plot(avg_acc_arr_val)
    plt.plot(avg_acc_arr_train)
    plt.legend(["validation accuracy", "training accuracy"], loc='upper left')

    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.show()
    
    model.load_state_dict(best_model_wts)
    return model

model = train_model(incept, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=600, aux_logits=aux_logits)
torch.save(incept.state_dict(), 'incept_multiclass.pt')

eval_model(incept, criterion)